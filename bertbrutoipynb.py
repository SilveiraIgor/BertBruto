# -*- coding: utf-8 -*-
"""BERTBrutoipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e-JuRUJZ5xVUPH8cyKVkRBR6DY5UAlD3
"""

!pip install -U datasets

import pandas as pd
import torch
from datasets import load_dataset
from sklearn.metrics import cohen_kappa_score

# prompt: gere uma classe em pytorch que possui dentro: um bert, uma camada que recebe o CLS do bert e gera outra representação em cima dela e por fim recebe essa representação e tem como output um número

import torch.nn as nn
from transformers import BertModel
from transformers import BertTokenizer



class BertForRegression(nn.Module):
    def __init__(self, bert_model_name='neuralmind/bert-base-portuguese-cased', hidden_size=768):
        super().__init__()
        self.bert = BertModel.from_pretrained(bert_model_name)
        self.linear_layer = nn.Linear(hidden_size, hidden_size)
        self.regression_layer = nn.Linear(hidden_size, 1, bias=False)

    def forward(self, essay1, essay2):
      with torch.no_grad():
        outputs1 = self.bert(**essay1)
        outputs2 = self.bert(**essay2)
      cls_representation1 = outputs1.last_hidden_state[:, 0, :]
      cls_representation2 = outputs2.last_hidden_state[:, 0, :]
      # Concatenate the representations
      intermediate_representation1 = self.linear_layer(cls_representation1)
      intermediate_representation2 = self.linear_layer(cls_representation2)
      #intermediate_representation1 = torch.relu(intermediate_representation1)
      #intermediate_representation2 = torch.relu(intermediate_representation2) # Optional activation
      dv = intermediate_representation1 - intermediate_representation2
      output = self.regression_layer(dv)
      y = torch.sigmoid(output)
      return y
rede = BertForRegression()
# Load the BERT tokenizer
tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')

ds = load_dataset("kamel-usp/aes_enem_dataset", "JBCS2025")

def normalizar(valor):
  return (valor + 5) / 10

def generate_training_samples(df_train):
  columns = ['essay1_text', 'essay2_text', 'label']
  df = pd.DataFrame(columns=columns)
  #df.head()
  print(len(df_train))
  for i in range(0, len(df_train)-2, 2):
    linha1 = df_train.iloc[i]
    linha2 = df_train.iloc[i + 1]
    nova_linha = pd.DataFrame([{"essay1_text": linha1['essay_text'], "essay2_text": linha2['essay_text'], 'label': normalizar((linha1['grades'][0] - linha2['grades'][0])//40) }])
    df = pd.concat([df, nova_linha], ignore_index=True)
  return df

#print(len(ds['test']))
#df_train = generate_training_samples(ds['test'].to_pandas())
#print(df_train)

# prompt: gere um training loop que recebe dataframe, tokeniza separadamente as chaves "essay1_text" e "essay2_text", passa os tokens para uma rede chamada rede e aplica uma regressão como loss

# Training loop function
def train_model(dataframe, model, tokenizer, epochs=1, batch_size=2, learning_rate=1e-5):
    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)
    criterion = nn.MSELoss()  # Mean Squared Error for regression

    for epoch in range(epochs):
        model.train()
        total_loss = 0

        # Create batches
        for i in range(0, len(dataframe), batch_size):
            batch = dataframe.iloc[i : i + batch_size]

            # Tokenize essay texts separately for each essay in the batch
            tokenized_essay1 = tokenizer(batch['essay1_text'].tolist(), padding=True, truncation=True, return_tensors="pt", max_length=512)
            tokenized_essay2 = tokenizer(batch['essay2_text'].tolist(), padding=True, truncation=True, return_tensors="pt", max_length=512)

            labels = torch.tensor(batch['label'].tolist(), dtype=torch.float32).unsqueeze(1)

            optimizer.zero_grad()

            # Pass tokenized inputs to the model
            outputs = model(essay1=tokenized_essay1, essay2=tokenized_essay2)

            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        print(f"Epoch {epoch+1}/{epochs}, Loss: {total_loss / (len(dataframe) / batch_size)}")

# Call the training loop
#train_model(df_train, rede, tokenizer)

# prompt: gere uma função que recebe um dataframe e seleciona aleatoriamente 10 instancias dele

def selecionar_aleatoriamente(dataframe_treino, instancia):
  columns = ['essay1_text', 'essay2_text', 'label']
  df = pd.DataFrame(columns=columns)
  aleatorio = pd.concat([df, dataframe_treino.sample(n=10)], ignore_index=True)
  for idx,linha1 in aleatorio.iterrows():
    nova_linha = pd.DataFrame([{"essay1_text": linha1['essay_text'], "essay2_text": instancia['essay_text'], 'label': linha1['grades'][0]//40 }])
    df = pd.concat([df, nova_linha], ignore_index=True)
  return df

selecionar_aleatoriamente(ds['train'].to_pandas(), ds['test'].to_pandas().iloc[0])

# prompt: faça uma função que recebe uma lista de inteiros e retorna o inteiro mais frequente da lista

def desnormalizar(valor):
  return 10 * valor - 5

def arredondar_notas(lista_notas, referencias):
  #print(lista_notas, referencias)
  novas = [j+desnormalizar(i) for i, j in zip(lista_notas, referencias)]
  nota = sum(novas)/len(novas)
  return round(nota)



def test_model(dataframe_train, dataframe_test, model, tokenizer):
      model.eval()
      total_loss = 0
      y = []
      y_pred = []
      # Create batches
      with torch.no_grad():
        for i in range(0, len(dataframe_test)):
            if i % 10 == 0 and i != 0:
                print(f"Exemplo {i} de {len(dataframe_test)}")
                #break
            batch = selecionar_aleatoriamente(dataframe_train, dataframe_test.iloc[i])
            # Tokenize essay texts separately for each essay in the batch
            tokenized_essay1 = tokenizer(batch['essay1_text'].tolist(), padding=True, truncation=True, return_tensors="pt", max_length=512)
            tokenized_essay2 = tokenizer(batch['essay2_text'].tolist(), padding=True, truncation=True, return_tensors="pt", max_length=512)
            outputs = model(essay1=tokenized_essay1, essay2=tokenized_essay2)
            y.append(dataframe_test.iloc[i]['grades'][0]//40)
            #print(f"O y: {y}")
            #print(outputs.flatten().tolist())
            arredondadas = arredondar_notas(outputs.flatten().tolist(), batch['label'].tolist())
            y_pred.append(arredondadas)
            #loss = criterion(outputs, labels
        #print(y)
        #print(y_pred)
        QWK = cohen_kappa_score(y, y_pred, labels=[0, 1, 2, 3, 4, 5], weights='quadratic')
        print(f">> Testando QWK: {QWK}")
#test_model(ds['train'].to_pandas(), ds['test'].to_pandas(), rede, tokenizer)



def treinar(ds, rede, tokenizer, epoch):
  df_train = generate_training_samples(ds['train'].to_pandas())
  for i in range(epoch):
    print(f"Epoca: {i+1}")
    train_model(df_train, rede, tokenizer)
    test_model(ds['train'].to_pandas(), ds['test'].to_pandas(), rede, tokenizer)
treinar(ds, rede, tokenizer, 5)



